{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "693641c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Entity Counts\n",
      "============================\n",
      "Label: ADR        | Distinct Entities: 3400\n",
      "Label: Disease    | Distinct Entities: 164\n",
      "Label: Drug       | Distinct Entities: 323\n",
      "Label: Symptom    | Distinct Entities: 148\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def aggregate_ann_entities(base_dir):\n",
    "    ann_dir = os.path.join(base_dir, 'original')\n",
    "    entities = defaultdict(set)\n",
    "    \n",
    "    if not os.path.exists(ann_dir):\n",
    "        print(f\" Error: Directory 'original' not found at {ann_dir}\")\n",
    "        return None\n",
    "\n",
    "    allowed_labels = {\"ADR\", \"Drug\", \"Disease\", \"Symptom\"}  \n",
    "\n",
    "    for file in os.listdir(ann_dir):\n",
    "        if file.endswith('.ann'):\n",
    "            full_path = os.path.join(ann_dir, file)\n",
    "            with open(full_path, 'r', encoding='utf-8') as stream:\n",
    "                for line in stream:\n",
    "                    if line.startswith('#') or not line.strip():\n",
    "                        continue\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) >= 3:\n",
    "                        cat = parts[1].split()[0]\n",
    "                        text_val = parts[2]\n",
    "                        if cat in allowed_labels:   \n",
    "                            entities[cat].add(text_val.lower())\n",
    "    return entities\n",
    "\n",
    "results = aggregate_ann_entities('.')\n",
    "if results is not None:\n",
    "    print(\"Distinct Entity Counts\")\n",
    "    print(\"=\" * 28)\n",
    "    for k, val in sorted(results.items()):\n",
    "        print(f\"Label: {k:<10} | Distinct Entities: {len(val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c8ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample Predictions (first 10):\n",
      "========================================\n",
      "T1\tSymptom 13 15\tdr\n",
      "T2\tSymptom 15 19\t##owsy\n",
      "T3\tSymptom 36 43\tblurred\n",
      "T4\tDisease 179 188\tarthritis\n",
      "T5\tSymptom 412 417\tpains\n",
      "\n",
      " BIO Output:\n",
      "========================================\n",
      "I\tO\n",
      "feel\tO\n",
      "a\tO\n",
      "bit\tO\n",
      "drowsy\tB-Symptom\n",
      "&\tO\n",
      "have\tO\n",
      "a\tO\n",
      "little\tO\n",
      "blurred\tB-Symptom\n",
      "vision,\tO\n",
      "so\tO\n",
      "far\tO\n",
      "no\tO\n",
      "gastric\tO\n",
      "problems.\tO\n",
      "I've\tO\n",
      "been\tO\n",
      "on\tO\n",
      "Arthrotec\tO\n",
      "50\tO\n",
      "for\tO\n",
      "over\tO\n",
      "10\tO\n",
      "years\tO\n",
      "on\tO\n",
      "and\tO\n",
      "off,\tO\n",
      "only\tO\n",
      "taking\tO\n",
      "it\tO\n",
      "when\tO\n",
      "I\tO\n",
      "needed\tO\n",
      "it.\tO\n",
      "Due\tO\n",
      "to\tO\n",
      "my\tO\n",
      "arthritis\tB-Disease\n",
      "getting\tO\n",
      "progressively\tO\n",
      "worse,\tO\n",
      "to\tO\n",
      "the\tO\n",
      "point\tO\n",
      "where\tO\n",
      "I\tO\n",
      "am\tO\n",
      "in\tO\n",
      "tears\tO\n",
      "with\tO\n",
      "the\tO\n",
      "agony,\tO\n",
      "gp's\tO\n",
      "started\tO\n",
      "me\tO\n",
      "on\tO\n",
      "75\tO\n",
      "twice\tO\n",
      "a\tO\n",
      "day\tO\n",
      "and\tO\n",
      "I\tO\n",
      "have\tO\n",
      "to\tO\n",
      "take\tO\n",
      "it.\tO\n",
      "every\tO\n",
      "day\tO\n",
      "for\t\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "#  Load pretrained biomedical NER model \n",
    "model_id = \"d4data/biomedical-ner-all\"\n",
    "ner_pipeline = pipeline(\"token-classification\", model=model_id, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Map labels \n",
    "translation = {\n",
    "    'Sign_symptom': 'Symptom',\n",
    "    'Disease_disorder': 'Disease',\n",
    "    'Adverse_drug_event': 'ADR',\n",
    "    'Medication': 'Drug'\n",
    "}\n",
    "\n",
    "#  BIO tagging function \n",
    "def convert_to_bio(text, predictions):\n",
    "    bio_output = []\n",
    "    # Create a list of 'O' for each token (initialize as Outside)\n",
    "    tokens = text.split()\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    for pred in predictions:\n",
    "        label = pred[\"label\"]\n",
    "        start, end, entity_text = pred[\"start\"], pred[\"end\"], pred[\"text\"]\n",
    "\n",
    "        # find matching tokens\n",
    "        for i, tok in enumerate(tokens):\n",
    "            # crude alignment by checking substring presence\n",
    "            if entity_text.lower() in tok.lower():\n",
    "                if labels[i] == \"O\":  # only overwrite if still O\n",
    "                    labels[i] = f\"B-{label}\"\n",
    "                else:\n",
    "                    labels[i] = f\"I-{label}\"\n",
    "\n",
    "    # Combine tokens with BIO labels\n",
    "    for t, l in zip(tokens, labels):\n",
    "        bio_output.append(f\"{t}\\t{l}\")\n",
    "    return \"\\n\".join(bio_output)\n",
    "\n",
    "\n",
    "#  Run prediction on sample file \n",
    "sample_txt = 'ARTHROTEC.1.txt'\n",
    "text_folder = os.path.join('.', 'text')\n",
    "text_path = os.path.join(text_folder, sample_txt)\n",
    "\n",
    "with open(text_path, 'r', encoding='utf-8') as file:\n",
    "    sample_data = file.read()\n",
    "\n",
    "raw_results = ner_pipeline(sample_data)\n",
    "\n",
    "# Map results \n",
    "mapped_predictions = []\n",
    "for item in raw_results:\n",
    "    new_label = translation.get(item['entity_group'])\n",
    "    if new_label:\n",
    "        pred = {\n",
    "            \"label\": new_label,\n",
    "            \"start\": item['start'],\n",
    "            \"end\": item['end'],\n",
    "            \"text\": item['word']\n",
    "        }\n",
    "        mapped_predictions.append(pred)\n",
    "\n",
    "#  Show first 10 predictions \n",
    "print(\"\\n Sample Predictions (first 10):\")\n",
    "print(\"=\" * 40)\n",
    "for i, p in enumerate(mapped_predictions[:10], start=1):\n",
    "    print(f\"T{i}\\t{p['label']} {p['start']} {p['end']}\\t{p['text']}\")\n",
    "\n",
    "# Convert predictions to BIO format \n",
    "print(\"\\n BIO Output:\")\n",
    "print(\"=\" * 40)\n",
    "bio_format = convert_to_bio(sample_data, mapped_predictions)\n",
    "print(bio_format[:500])  # show first 500 chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a5f35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Task 3 & 4\n",
      "\n",
      " Performance for 'ARTHROTEC.1.ann' (All Entities):\n",
      "  - Precision: 40.00%\n",
      "  - Recall:    25.00%\n",
      "  - F1-Score:  30.77%\n",
      "\n",
      " Per-Label Performance:\n",
      "  Disease  | P: 100.00%  R: 100.00%  F1: 100.00%\n",
      "  Symptom  | P: 25.00%  R: 50.00%  F1: 33.33%\n",
      "  Drug     | P: 0.00%  R: 0.00%  F1: 0.00%\n",
      "  ADR      | P: 0.00%  R: 0.00%  F1: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\" Task 3 & 4\")\n",
    "\n",
    "def load_ground_truth(file_path):\n",
    "    gt_set = set()\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Warning: Ground truth file missing at {file_path}\")\n",
    "        return gt_set\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('T'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) > 1:\n",
    "                    details = parts[1].split()\n",
    "                    if len(details) >= 3:\n",
    "                        label = details[0]\n",
    "                        start, end = int(details[1]), int(details[-1])\n",
    "                        gt_set.add((label, start, end))\n",
    "    return gt_set\n",
    "\n",
    "def compute_metrics(preds, truths):\n",
    "    # Convert predictions into set for exact matching\n",
    "    pred_set = {(p['label'], p['start'], p['end']) for p in preds}\n",
    "\n",
    "    tp = len(pred_set.intersection(truths))\n",
    "    fp = len(pred_set - truths)\n",
    "    fn = len(truths - pred_set)\n",
    "    \n",
    "    precision_val = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall_val = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_val = 2 * precision_val * recall_val / (precision_val + recall_val) if (precision_val + recall_val) > 0 else 0\n",
    "    \n",
    "    return {\"precision\": precision_val, \"recall\": recall_val, \"f1_score\": f1_val}\n",
    "\n",
    "def compute_metrics_per_label(preds, truths):\n",
    "    # Group ground truths by label\n",
    "    truth_by_label = defaultdict(set)\n",
    "    for t in truths:\n",
    "        truth_by_label[t[0]].add(t)\n",
    "\n",
    "    # Group predictions by label\n",
    "    pred_by_label = defaultdict(set)\n",
    "    for p in preds:\n",
    "        pred_by_label[p['label']].add((p['label'], p['start'], p['end']))\n",
    "\n",
    "    results = {}\n",
    "    for label in set(list(truth_by_label.keys()) + list(pred_by_label.keys())):\n",
    "        tp = len(pred_by_label[label].intersection(truth_by_label[label]))\n",
    "        fp = len(pred_by_label[label] - truth_by_label[label])\n",
    "        fn = len(truth_by_label[label] - pred_by_label[label])\n",
    "        \n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0\n",
    "        results[label] = {\"precision\": prec, \"recall\": rec, \"f1_score\": f1}\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run on single file \n",
    "gt_filename = 'ARTHROTEC.1.ann'\n",
    "ann_directory = os.path.join('.', 'original')\n",
    "gt_file_path = os.path.join(ann_directory, gt_filename)\n",
    "ground_truth = load_ground_truth(gt_file_path)\n",
    "\n",
    "# Overall metrics\n",
    "metrics = compute_metrics(mapped_predictions, ground_truth)\n",
    "print(f\"\\n Performance for '{gt_filename}' (All Entities):\")\n",
    "print(f\"  - Precision: {metrics['precision']:.2%}\")\n",
    "print(f\"  - Recall:    {metrics['recall']:.2%}\")\n",
    "print(f\"  - F1-Score:  {metrics['f1_score']:.2%}\")\n",
    "\n",
    "# Per-label metrics\n",
    "print(\"\\n Per-Label Performance:\")\n",
    "metrics_per_label = compute_metrics_per_label(mapped_predictions, ground_truth)\n",
    "for lbl, vals in metrics_per_label.items():\n",
    "    print(f\"  {lbl:<8} | P: {vals['precision']:.2%}  R: {vals['recall']:.2%}  F1: {vals['f1_score']:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc39304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running Task 5: Performance on 50 Random Posts \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Files: 100%|██████████| 50/50 [00:09<00:00,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Overall Performance on 50 Random Files:\n",
      "  - Precision: 7.20%\n",
      "  - Recall:    5.28%\n",
      "  - F1-Score:  6.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"\\n Running Task 5: Performance on 50 Random Posts \")\n",
    "\n",
    "def parse_ground_truth(filepath):\n",
    "    \"\"\"Reads a .ann file and returns a set of (label, start, end) tuples.\"\"\"\n",
    "    ground_truths = set()\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Warning: File {filepath} not found.\")\n",
    "        return ground_truths\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('T'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) > 1:\n",
    "                    label_info = parts[1].split()\n",
    "                    if len(label_info) >= 3:\n",
    "                        label = label_info[0]\n",
    "                        start = int(label_info[1])\n",
    "                        end = int(label_info[-1])\n",
    "                        ground_truths.add((label, start, end))\n",
    "    return ground_truths\n",
    "\n",
    "# Directories \n",
    "text_dir = os.path.join('.', 'text')\n",
    "original_dir = os.path.join('.', 'original')\n",
    "\n",
    "# Retrieve the list of .txt files\n",
    "all_files = [f for f in os.listdir(text_dir) if f.endswith('.txt')]\n",
    "random.seed(42)  # for reproducibility\n",
    "sample_size = min(50, len(all_files))\n",
    "selected_files = random.sample(all_files, sample_size)\n",
    "\n",
    "total_tp, total_fp, total_fn = 0, 0, 0\n",
    "\n",
    "for filename in tqdm(selected_files, desc=\"Evaluating Files\"):\n",
    "    # Read the current text file\n",
    "    file_path = os.path.join(text_dir, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Get model predictions using ner_pipeline (from Task 2)\n",
    "    predictions = ner_pipeline(text)\n",
    "    \n",
    "    # Format predictions into a set of (label, start, end) tuples\n",
    "    formatted_preds = set()\n",
    "    for entity in predictions:\n",
    "        entity_label = translation.get(entity['entity_group'])\n",
    "        if entity_label:\n",
    "            formatted_preds.add((entity_label, entity['start'], entity['end']))\n",
    "    \n",
    "    # Obtain ground truth from the corresponding .ann file\n",
    "    ann_filename = filename.replace('.txt', '.ann')\n",
    "    gt_filepath = os.path.join(original_dir, ann_filename)\n",
    "    ground_truths = parse_ground_truth(gt_filepath)\n",
    "    \n",
    "    # Update true positives, false positives, and false negatives\n",
    "    total_tp += len(formatted_preds.intersection(ground_truths))\n",
    "    total_fp += len(formatted_preds - ground_truths)\n",
    "    total_fn += len(ground_truths - formatted_preds)\n",
    "\n",
    "# Calculate micro-averaged metrics\n",
    "overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "overall_f1 = (2 * overall_precision * overall_recall / (overall_precision + overall_recall)\n",
    "              if (overall_precision + overall_recall) > 0 else 0)\n",
    "\n",
    "print(f\"\\n Overall Performance on {sample_size} Random Files:\")\n",
    "print(f\"  - Precision: {overall_precision:.2%}\")\n",
    "print(f\"  - Recall:    {overall_recall:.2%}\")\n",
    "print(f\"  - F1-Score:  {overall_f1:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c4f201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running Task 6: Combining Data and Entity Matching \n",
      "Model found no ADRs in the sample text. Cannot perform matching for this file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "print(\"\\n Running Task 6: Combining Data and Entity Matching \")\n",
    "\n",
    "# Check that mapped_predictions is available\n",
    "try:\n",
    "    _ = mapped_predictions\n",
    "except NameError:\n",
    "    raise NameError(\"mapped_predictions is not defined.\")\n",
    "\n",
    "# Define file paths\n",
    "sample_filename_ann = 'ARTHROTEC.1.ann'\n",
    "sct_dir = os.path.join('cadec', 'sct')\n",
    "original_dir = os.path.join('cadec', 'original')\n",
    "sct_file = os.path.join(sct_dir, sample_filename_ann)\n",
    "original_file = os.path.join(original_dir, sample_filename_ann)\n",
    "\n",
    "def create_combined_data_structure(original_ann_path, sct_ann_path):\n",
    "    \n",
    "    original_data = {}\n",
    "    if os.path.exists(original_ann_path):\n",
    "        with open(original_ann_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('T'):\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    original_data[parts[0]] = {\n",
    "                        'label_type': parts[1].split()[0],\n",
    "                        'text_segment': parts[2]\n",
    "                    }\n",
    "\n",
    "    combined_list = []\n",
    "    if os.path.exists(sct_ann_path):\n",
    "        with open(sct_ann_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('T'):\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    t_id = parts[0]\n",
    "                    if t_id in original_data:\n",
    "                        try:\n",
    "                            info = parts[1].split('\"')\n",
    "                            code = info[0].split()[-1]\n",
    "                            description = info[1]\n",
    "                            entry = original_data[t_id]\n",
    "                            entry['snomed_code'] = code\n",
    "                            entry['snomed_description'] = description\n",
    "                            combined_list.append(entry)\n",
    "                        except IndexError:\n",
    "                            continue  # Skip malformed SCT lines\n",
    "    return combined_list\n",
    "\n",
    "# Merge data\n",
    "combined_data = create_combined_data_structure(original_file, sct_file)\n",
    "\n",
    "# Ground truth ADRs (with SNOMED mappings)\n",
    "ground_truth_adrs = [d for d in combined_data if d['label_type'] == 'ADR']\n",
    "\n",
    "# Model predictions ADRs\n",
    "model_predicted_adrs = [p for p in mapped_predictions if p['label'] == 'ADR']\n",
    "\n",
    "# Guard checks\n",
    "if not model_predicted_adrs:\n",
    "    print(\"Model found no ADRs in the sample text. Cannot perform matching for this file.\")\n",
    "elif not ground_truth_adrs:\n",
    "    print(\"No ground truth ADRs with SNOMED codes found in the sample file. Cannot perform matching.\")\n",
    "else:\n",
    "    snomed_descriptions = [d['snomed_description'].strip().lower() for d in ground_truth_adrs if 'snomed_description' in d]\n",
    "    if not snomed_descriptions:\n",
    "        print(\"Ground truth ADRs exist but no valid SNOMED descriptions found.\")\n",
    "    else:\n",
    "        print(\"Loading embedding model (this may take a moment)...\")\n",
    "        embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "        # Encode all descriptions at once\n",
    "        description_embeddings = embedding_model.encode(snomed_descriptions, convert_to_tensor=True)\n",
    "\n",
    "        # Batch encode predictions\n",
    "        pred_texts = [p['text'].strip().lower() for p in model_predicted_adrs]\n",
    "        pred_embeddings = embedding_model.encode(pred_texts, convert_to_tensor=True)\n",
    "\n",
    "        print(\"Models ready. Performing matching...\")\n",
    "\n",
    "        # Iterate over predictions and compare\n",
    "        for i, adr_pred in enumerate(model_predicted_adrs):\n",
    "            pred_text = pred_texts[i]\n",
    "            print(f\"\\n--- Matching for Predicted ADR: '{adr_pred['text']}' ---\")\n",
    "\n",
    "            # a) Fuzzy String Matching\n",
    "            fuzzy_scores = [fuzz.token_set_ratio(pred_text, desc) for desc in snomed_descriptions]\n",
    "            best_fuzzy_idx = int(max(range(len(fuzzy_scores)), key=fuzzy_scores.__getitem__))\n",
    "            best_fuzzy_match = ground_truth_adrs[best_fuzzy_idx]\n",
    "\n",
    "            # b) Embedding Matching\n",
    "            cosine_scores = util.cos_sim(pred_embeddings[i], description_embeddings)[0]\n",
    "            best_embedding_idx = int(cosine_scores.argmax())\n",
    "            best_embedding_match = ground_truth_adrs[best_embedding_idx]\n",
    "\n",
    "            print(\"  String Match Result:    '{}' (Score: {})\".format(\n",
    "                best_fuzzy_match[\"snomed_description\"], fuzzy_scores[best_fuzzy_idx]))\n",
    "            print(\"  Embedding Match Result: '{}' (Score: {:.2f})\".format(\n",
    "                best_embedding_match[\"snomed_description\"], float(cosine_scores[best_embedding_idx])))\n",
    "\n",
    "            if best_fuzzy_match['snomed_code'] == best_embedding_match['snomed_code']:\n",
    "                print(\"  Comparison: Both methods agree.\")\n",
    "            else:\n",
    "                print(\"  Comparison: Methods disagree. Embedding match is often semantically superior.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4bcbb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
